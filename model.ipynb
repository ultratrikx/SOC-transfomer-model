{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "import timm\n",
    "\n",
    "########################################\n",
    "# Configuration\n",
    "########################################\n",
    "landsat_dir = '/landsat_model'  # contains sceneName_B1.tif ... sceneName_B6.tif\n",
    "soil_dir = '/soilgrid_model'        # contains sceneName_B1.tif for soil\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "threshold = None  # Will set this after we have training data stats\n",
    "\n",
    "########################################\n",
    "# Dataset\n",
    "########################################\n",
    "class SoilCarbonDataset(Dataset):\n",
    "    def __init__(self, landsat_dir, soil_dir, scene_list):\n",
    "        self.landsat_dir = landsat_dir\n",
    "        self.soil_dir = soil_dir\n",
    "        self.scenes = scene_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.scenes[idx]\n",
    "\n",
    "        # Read the 6 Landsat bands\n",
    "        # Landsat bands\n",
    "        landsat_bands = []\n",
    "        landsat_files = sorted(glob.glob(os.path.join(self.landsat_dir, f\"{scene}*B[1-6].tif\")))\n",
    "        for band_file in landsat_files:\n",
    "            with rasterio.open(band_file) as src:\n",
    "                band_data = src.read(1)  # shape: (80,80)\n",
    "                landsat_bands.append(band_data)\n",
    "        landsat_img = np.stack(landsat_bands, axis=0)  # shape: (6,80,80)\n",
    "\n",
    "        # Soil data (single band)\n",
    "        soil_file = glob.glob(os.path.join(self.soil_dir, f\"{scene}*B1.tif\"))[0]\n",
    "        with rasterio.open(soil_file) as src:\n",
    "            soil_data = src.read(1)  # shape: (80,80) or whatever size\n",
    "        soil_val = np.mean(soil_data)\n",
    "\n",
    "        landsat_img = torch.from_numpy(landsat_img).float()  # (6,80,80)\n",
    "        soil_val = torch.tensor(soil_val).float()\n",
    "\n",
    "        return landsat_img, soil_val\n",
    "\n",
    "########################################\n",
    "# Prepare Data\n",
    "########################################\n",
    "# Assume each scene is identified by a unique base name (e.g. scene_001)\n",
    "# We'll find all scenes by listing Landsat directory and extracting the base names.\n",
    "landsat_files = glob.glob(os.path.join(landsat_dir, \"*_B1.tif\"))\n",
    "scenes = [os.path.basename(f).replace(\"_B1.tif\",\"\") for f in landsat_files]\n",
    "\n",
    "dataset = SoilCarbonDataset(landsat_dir, soil_dir, scenes)\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Compute threshold for binary classification metrics (F1, Kappa)\n",
    "# We'll use median soil value from the training set\n",
    "all_train_soil_vals = []\n",
    "for i in range(len(train_dataset)):\n",
    "    # get train dataset indices\n",
    "    landsat_img, soil_val = train_dataset[i]\n",
    "    all_train_soil_vals.append(soil_val.item())\n",
    "threshold = np.median(all_train_soil_vals)\n",
    "print(f\"Using threshold={threshold} for F1 and Kappa computation.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "########################################\n",
    "# Model\n",
    "########################################\n",
    "# Create ViT model from timm and adapt first layer for 6 channels and last layer for regression\n",
    "model = timm.create_model('vit_small_patch16_224', pretrained=False, num_classes=1)\n",
    "\n",
    "# Adapt input conv layer to 6 channels (instead of 3)\n",
    "original_conv = model.patch_embed.proj\n",
    "model.patch_embed.proj = nn.Conv2d(6, original_conv.out_channels, \n",
    "                                   kernel_size=original_conv.kernel_size,\n",
    "                                   stride=original_conv.stride,\n",
    "                                   padding=original_conv.padding)\n",
    "\n",
    "# The model expects 224x224 input. We have 80x80.\n",
    "# For simplicity, resize input in the forward pass (quick hack).\n",
    "# A better solution: modify model.patch_embed.patch_size or use another model more suited to 80x80.\n",
    "resize = nn.Upsample(size=(224,224), mode='bilinear', align_corners=False)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "########################################\n",
    "# Loss and Optimizer\n",
    "########################################\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "########################################\n",
    "# Utility functions for metrics\n",
    "########################################\n",
    "def compute_metrics(y_true, y_pred, threshold):\n",
    "    # y_true, y_pred are arrays of floats\n",
    "    # Convert to binary\n",
    "    y_true_bin = (y_true >= threshold).astype(int)\n",
    "    y_pred_bin = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    f1 = f1_score(y_true_bin, y_pred_bin)\n",
    "    kappa = cohen_kappa_score(y_true_bin, y_pred_bin)\n",
    "\n",
    "    return mse, f1, kappa\n",
    "\n",
    "########################################\n",
    "# Training Loop\n",
    "########################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for landsat, soil_val in train_loader:\n",
    "        landsat = landsat.cuda()  # (B,6,80,80)\n",
    "        soil_val = soil_val.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        landsat_resized = resize(landsat)  # (B,6,224,224)\n",
    "        preds = model(landsat_resized)  # (B,1)\n",
    "        loss = criterion(preds.squeeze(), soil_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_val_true = []\n",
    "    all_val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for landsat, soil_val in val_loader:\n",
    "            landsat = landsat.cuda()\n",
    "            soil_val = soil_val.cuda()\n",
    "            landsat_resized = resize(landsat)\n",
    "            preds = model(landsat_resized)\n",
    "            \n",
    "            all_val_true.append(soil_val.cpu().numpy())\n",
    "            all_val_pred.append(preds.squeeze().cpu().numpy())\n",
    "\n",
    "    all_val_true = np.concatenate(all_val_true)\n",
    "    all_val_pred = np.concatenate(all_val_pred)\n",
    "    mse_val, f1_val, kappa_val = compute_metrics(all_val_true, all_val_pred, threshold)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Val MSE={mse_val:.4f}, F1={f1_val:.4f}, Kappa={kappa_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
