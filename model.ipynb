{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 scenes\n",
      "Using threshold=332.0718688964844 for F1 and Kappa computation.\n",
      "Epoch 1/10: Train Loss=10160107.3625, Val Loss=9360532.5969, Val MSE=9360532.0000, F1=0.0000, Kappa=0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "import timm\n",
    "\n",
    "########################################\n",
    "# Configuration\n",
    "########################################\n",
    "# Use relative or absolute paths to your data directories\n",
    "landsat_dir = 'landsat_model'  # contains sceneName_B1.tif ... sceneName_B6.tif\n",
    "soil_dir = 'soilgrid_model'    # contains sceneName_B1.tif for soil\n",
    "\n",
    "# Check if directories exist\n",
    "if not os.path.exists(landsat_dir):\n",
    "    raise FileNotFoundError(f\"Landsat directory not found: {landsat_dir}\")\n",
    "if not os.path.exists(soil_dir):\n",
    "    raise FileNotFoundError(f\"Soil directory not found: {soil_dir}\")\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "threshold = None  # Will set this after we have training data stats\n",
    "\n",
    "########################################\n",
    "# Dataset\n",
    "########################################\n",
    "class SoilCarbonDataset(Dataset):\n",
    "    def __init__(self, landsat_dir, soil_dir, scene_list):\n",
    "        self.landsat_dir = landsat_dir\n",
    "        self.soil_dir = soil_dir\n",
    "        self.scenes = scene_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.scenes[idx]\n",
    "    \n",
    "        # Read the 6 Landsat bands\n",
    "        landsat_bands = []\n",
    "        for band_num in [1, 2, 3, 4, 5, 7]:  # bands 1-5,7\n",
    "            band_pattern = os.path.join(self.landsat_dir, f\"{scene}_*_b{band_num}.tif\")\n",
    "            band_files = glob.glob(band_pattern)\n",
    "            if not band_files:\n",
    "                raise FileNotFoundError(f\"Band {band_num} not found for scene {scene}\")\n",
    "            with rasterio.open(band_files[0]) as src:\n",
    "                band_data = src.read(1)  # shape: (80,80)\n",
    "                landsat_bands.append(band_data)\n",
    "        \n",
    "        # Stack Landsat bands\n",
    "        landsat_img = np.stack(landsat_bands, axis=0)  # shape: (6,80,80)\n",
    "        \n",
    "        # Read soil data\n",
    "        soil_pattern = os.path.join(self.soil_dir, f\"{scene}_*_s1.tif\")\n",
    "        soil_files = glob.glob(soil_pattern)\n",
    "        if not soil_files:\n",
    "            raise FileNotFoundError(f\"Soil data not found for scene {scene}\")\n",
    "        with rasterio.open(soil_files[0]) as src:\n",
    "            soil_data = src.read(1).astype(np.float32)  # ensure float32\n",
    "        soil_val = np.mean(soil_data)\n",
    "\n",
    "        # Convert to tensors\n",
    "        landsat_img = torch.from_numpy(landsat_img).float()  # (6,80,80)\n",
    "        soil_val = torch.tensor(soil_val).float()\n",
    "        \n",
    "        return landsat_img, soil_val\n",
    "    \n",
    "########################################\n",
    "# Prepare Data\n",
    "########################################\n",
    "# Find all scenes by listing Landsat directory and extracting the base scene numbers\n",
    "landsat_files = glob.glob(os.path.join(landsat_dir, \"*_b1.tif\"))\n",
    "if not landsat_files:\n",
    "    raise FileNotFoundError(f\"No Landsat files found in {landsat_dir}\")\n",
    "scenes = [os.path.basename(f).split('_')[0] for f in landsat_files]\n",
    "print(f\"Found {len(scenes)} scenes\")\n",
    "\n",
    "dataset = SoilCarbonDataset(landsat_dir, soil_dir, scenes)\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Compute threshold for binary classification metrics (F1, Kappa)\n",
    "# We'll use median soil value from the training set\n",
    "all_train_soil_vals = []\n",
    "for i in range(len(train_dataset)):\n",
    "    # get train dataset indices\n",
    "    landsat_img, soil_val = train_dataset[i]\n",
    "    all_train_soil_vals.append(soil_val.item())\n",
    "threshold = np.median(all_train_soil_vals)\n",
    "print(f\"Using threshold={threshold} for F1 and Kappa computation.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "########################################\n",
    "# Model\n",
    "########################################\n",
    "# Create ViT model from timm and adapt first layer for 6 channels and last layer for regression\n",
    "model = timm.create_model('vit_small_patch16_224', pretrained=False, num_classes=1)\n",
    "\n",
    "# Adapt input conv layer to 6 channels (instead of 3)\n",
    "original_conv = model.patch_embed.proj\n",
    "model.patch_embed.proj = nn.Conv2d(6, original_conv.out_channels, \n",
    "                                   kernel_size=original_conv.kernel_size,\n",
    "                                   stride=original_conv.stride,\n",
    "                                   padding=original_conv.padding)\n",
    "\n",
    "# The model expects 224x224 input. We have 80x80.\n",
    "# For simplicity, resize input in the forward pass (quick hack).\n",
    "# A better solution: modify model.patch_embed.patch_size or use another model more suited to 80x80.\n",
    "resize = nn.Upsample(size=(224,224), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Check if CUDA is available and use it, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "########################################\n",
    "# Loss and Optimizer\n",
    "########################################\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "########################################\n",
    "# Utility functions for metrics\n",
    "########################################\n",
    "def compute_metrics(y_true, y_pred, threshold):\n",
    "    # y_true, y_pred are arrays of floats\n",
    "    # Convert to binary\n",
    "    y_true_bin = (y_true >= threshold).astype(int)\n",
    "    y_pred_bin = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    f1 = f1_score(y_true_bin, y_pred_bin)\n",
    "    kappa = cohen_kappa_score(y_true_bin, y_pred_bin)\n",
    "\n",
    "    return mse, f1, kappa\n",
    "\n",
    "########################################\n",
    "# Training Loop\n",
    "########################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for landsat, soil_val in train_loader:\n",
    "        landsat = landsat.to(device)  # (B,6,80,80)\n",
    "        soil_val = soil_val.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        landsat_resized = resize(landsat)  # (B,6,224,224)\n",
    "        preds = model(landsat_resized)  # (B,1)\n",
    "        loss = criterion(preds.squeeze(), soil_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_val_true = []\n",
    "    all_val_pred = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for landsat, soil_val in val_loader:\n",
    "            landsat = landsat.to(device)\n",
    "            soil_val = soil_val.to(device)\n",
    "            landsat_resized = resize(landsat)\n",
    "            preds = model(landsat_resized)\n",
    "            val_loss += criterion(preds.squeeze(), soil_val).item()\n",
    "            \n",
    "            all_val_true.append(soil_val.cpu().numpy())\n",
    "            all_val_pred.append(preds.squeeze().cpu().numpy())\n",
    "\n",
    "    all_val_true = np.concatenate(all_val_true)\n",
    "    all_val_pred = np.concatenate(all_val_pred)\n",
    "    mse_val, f1_val, kappa_val = compute_metrics(all_val_true, all_val_pred, threshold)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val MSE={mse_val:.4f}, F1={f1_val:.4f}, Kappa={kappa_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
